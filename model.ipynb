{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fd8857",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71fd8857",
    "outputId": "4a5d4aef-a438-4bd0-a75e-2364e3b20069",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install spektral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eee543e",
   "metadata": {
    "id": "8eee543e"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import keras\n",
    "from keras import models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import keras.backend as K\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from spektral.layers import GCNConv, GlobalSumPool, GraphSageConv\n",
    "import skimage.measure\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from spektral.data import Graph\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m27kRgzFKZr5",
   "metadata": {
    "id": "m27kRgzFKZr5"
   },
   "source": [
    "labels (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "_5niblLrJzax",
   "metadata": {
    "id": "_5niblLrJzax"
   },
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"/home/memo/SemEval2020/preprocessed/df_combined_13892.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BPhPxXSUKkHL",
   "metadata": {
    "id": "BPhPxXSUKkHL"
   },
   "source": [
    "labels (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sl44lUVvFeFF",
   "metadata": {
    "id": "sl44lUVvFeFF"
   },
   "outputs": [],
   "source": [
    "dataframe_test = pd.read_csv(\"/home/memo/SemEval2020/preprocessed/labels_preprocessed_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zuyNakYJKeUs",
   "metadata": {
    "id": "zuyNakYJKeUs"
   },
   "source": [
    "CLIP embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sptw-bzKtG5E",
   "metadata": {
    "id": "sptw-bzKtG5E"
   },
   "outputs": [],
   "source": [
    "text_embd = np.load(\"/home/memo/SemEval2020/preprocessed/textencodings/text_clip_prepr.npy\")\n",
    "img_embd = np.load(\"/home/memo/SemEval2020/preprocessed/imageencodings/image_clip_prepr.npy\")\n",
    "\n",
    "text_embd_aug = np.load(\"/home/memo/SemEval2020/preprocessed/textencodings/text_clip_aug_train.npy\")\n",
    "img_embd_aug = np.load(\"/home/memo/SemEval2020/preprocessed/imageencodings/image_clip_aug_train.npy\")\n",
    "\n",
    "text_embd_new = np.concatenate((text_embd,text_embd_aug), axis=0)\n",
    "img_embd_new = np.concatenate((img_embd,img_embd_aug), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ZhYNytQXtG5E",
   "metadata": {
    "id": "ZhYNytQXtG5E"
   },
   "outputs": [],
   "source": [
    "text_test = np.load(\"/home/memo/SemEval2020/preprocessed/textencodings/text_clip_prepr_test.npy\")\n",
    "img_test = np.load(\"/home/memo/SemEval2020/preprocessed/imageencodings/image_clip_prepr_test.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WdICPvxhRX4M",
   "metadata": {
    "id": "WdICPvxhRX4M"
   },
   "source": [
    "# **Generate train labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "peaIposIIReK",
   "metadata": {
    "id": "peaIposIIReK"
   },
   "outputs": [],
   "source": [
    "def get_train_labels(df, task, cat=0):\n",
    "\n",
    "  if task == 'A':\n",
    "    def task_A_labels(dframe):\n",
    "      def fun_A(text):\n",
    "        if text=='positive' or text=='very_positive':\n",
    "          return 'positive'\n",
    "        if text=='negative' or text=='very_negative':\n",
    "          return 'negative'\n",
    "        return 'neutral'\n",
    "\n",
    "      taskA_labels = dframe.overall_sentiment.apply(fun_A)\n",
    "      return pd.get_dummies(taskA_labels)\n",
    "\n",
    "    return np.array(task_A_labels(df)).astype(int)\n",
    "\n",
    "\n",
    "  if task == 'B':\n",
    "    if cat == 'humour':\n",
    "      x = 'not_funny'\n",
    "\n",
    "    elif cat == 'sarcasm':\n",
    "      x = 'not_sarcastic'\n",
    "\n",
    "    elif cat == 'offensive':\n",
    "      x = 'not_offensive'\n",
    "\n",
    "    elif cat == 'motivational':\n",
    "      x = 'not_motivational'\n",
    "\n",
    "    else:\n",
    "      print('invalid cat')\n",
    "\n",
    "    def func(text):\n",
    "      if text == x:\n",
    "        return 0\n",
    "      return 1\n",
    "\n",
    "    return np.array(pd.get_dummies(df[cat].apply(func))).astype(int)\n",
    "\n",
    "  elif task == 'C':\n",
    "      if cat == 'humour':\n",
    "        labels_ohe = pd.get_dummies(df[cat])\n",
    "        return np.array(labels_ohe[['not_funny', 'funny', 'very_funny', 'hilarious']]).astype(int)\n",
    "\n",
    "      elif cat == 'sarcasm':\n",
    "        labels_ohe = pd.get_dummies(df[cat])\n",
    "        return np.array(labels_ohe[['not_sarcastic', 'general', 'twisted_meaning', 'very_twisted']]).astype(int)\n",
    "\n",
    "      elif cat == 'offensive':\n",
    "        labels_ohe = pd.get_dummies(df[cat])\n",
    "        return np.array(labels_ohe[['not_offensive', 'slight', 'very_offensive', 'hateful_offensive']]).astype(int)\n",
    "\n",
    "      elif cat == 'motivational':\n",
    "        def func(text):\n",
    "          if text == 'not_motivational':\n",
    "            return 0\n",
    "          return 1\n",
    "        return np.array(pd.get_dummies(df[cat].apply(func))).astype(int)\n",
    "\n",
    "      else:\n",
    "        print('invalid category')\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rOb_MZ9VdipX",
   "metadata": {
    "id": "rOb_MZ9VdipX"
   },
   "source": [
    "# **Generate test labels**\n",
    "\n",
    "Task A:\n",
    "\n",
    "For task a: the scores can be one of [-1, 0, 1]. For task b, the scores can be one of [0, 1]. For task c, the scores can be one of [0, 1, 2, 3].\n",
    "The four digits for task-b and task-c should be in the following order: humor, sarcasm, offensive, motivational.\n",
    "\n",
    "Task B:\n",
    "*   Not humorous => 0 and Humorous (funny, very funny, hilarious) => 1\n",
    "*   Not Sarcastic => 0 and Sarcastic (general, twisted meaning, very twisted) => 1\n",
    "*   Not offensive => 0 and Offensive (slight, very offensive, hateful offensive) => 1\n",
    "*   Not Motivational => 0 and Motivational => 1\n",
    "\n",
    "Task C:\n",
    "\n",
    "Humour :\n",
    "Not funny => 0\n",
    "Funny => 1\n",
    "Very funny => 2\n",
    "Hilarious => 3\n",
    "\n",
    "Sarcasm:\n",
    "Not Sarcastic => 0\n",
    "General => 1\n",
    "Twisted Meaning => 2\n",
    "Very Twisted => 3\n",
    "\n",
    "Offense:\n",
    "Not offensive => 0\n",
    "Slight => 1\n",
    "Very Offensive => 2\n",
    "Hateful Offensive => 3\n",
    "\n",
    "Motivation:\n",
    "Not Motivational => 0\n",
    "Motivational => 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "sv-JJ2ks9KFm",
   "metadata": {
    "id": "sv-JJ2ks9KFm"
   },
   "outputs": [],
   "source": [
    "def get_test_labels(df_test, task, cat=0):\n",
    "  if cat == 'humour':\n",
    "    def fun(num):\n",
    "      return int(num/1000)\n",
    "\n",
    "  elif cat == 'sarcasm':\n",
    "    def fun(num):\n",
    "      return int((num%1000)/100)\n",
    "\n",
    "  elif cat == 'offensive':\n",
    "    def fun(num):\n",
    "      return int((num%100)/10)\n",
    "\n",
    "  elif cat == 'motivational':\n",
    "    def fun(num):\n",
    "      return int(num%10)\n",
    "    if task=='C':\n",
    "      df_test[cat+'_taskC'] = df_test['T3'].apply(fun)\n",
    "      return np.array(pd.get_dummies(df_test['T3'].apply(fun))).astype(int)\n",
    "\n",
    "  elif task=='A':\n",
    "    return np.array(pd.get_dummies(df_test['T1'])).astype(int)\n",
    "\n",
    "  else:\n",
    "    print('invalid cat')\n",
    "    return\n",
    "\n",
    "  if task=='B':\n",
    "    df_test[cat+'_taskB'] = df_test['T2'].apply(fun)\n",
    "    return np.array(pd.get_dummies(df_test[cat+'_taskB'])).astype(int)\n",
    "\n",
    "  elif task=='C':\n",
    "    df_test[cat+'_taskC'] = df_test['T3'].apply(fun)\n",
    "    return np.array(pd.get_dummies(df_test[cat+'_taskC'])).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a10f8eb8",
   "metadata": {
    "id": "a10f8eb8"
   },
   "outputs": [],
   "source": [
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00ad77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"path/to/output/file\"\n",
    "with open(filename, \"a\") as file:\n",
    "    file.write(f\"Task, Category, Accuracy, P_macro, R_macro, F1_macro\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c7ed975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read arrays\n",
    "df = pd.read_csv(\"/home/memo/SemEval2020/Dataset/memotion_dataset_7k/labels_new.csv\")\n",
    "# clip\n",
    "img_glob = np.load(\"/home/memo/SemEval2020/imageencodings/image_clip_original_6986.npy\")\n",
    "text_glob = np.load(\"/home/memo/SemEval2020/textencodings/text_clip_original_6986.npy\")\n",
    "# altclip\n",
    "img_glob_alt = np.random.rand(6986, 100, 2048)\n",
    "text_glob_alt = np.random.rand(6986, 128, 768)\n",
    "# toxcity_roberta, nrclex, stanford_corenlp\n",
    "toxicity = np.load(\"/home/memo/SemEval2020/additional_features/roberta_avg_pool_6986.npy\")\n",
    "nrclex = np.load(\"/home/memo/SemEval2020/additional_features/nrclex_6986.npy\")\n",
    "stanford = np.load(\"/home/memo/SemEval2020/additional_features/corenlp_6986.npy\")\n",
    "# concatenate other features in the starting only\n",
    "additional_features = np.concatenate((toxicity, nrclex,stanford), axis=1)\n",
    "\n",
    "\n",
    "df_test = pd.read_csv(\"/home/memo/SemEval2020/Dataset/test/labels_test_new.csv\")\n",
    "# clip\n",
    "img_glob_test = np.load(\"/home/memo/SemEval2020/imageencodings/image_clip_original_6986_test.npy\")\n",
    "text_glob_test = np.load(\"/home/memo/SemEval2020/textencodings/text_clip_original_6986_test.npy\")\n",
    "# altclip\n",
    "img_glob_alt_test = np.random.rand(1860, 100, 2048)\n",
    "text_glob_alt_test = np.random.rand(1860, 128, 768)\n",
    "# toxcity_roberta, nrclex, stanford_corenlp\n",
    "toxicity_test = np.load(\"/home/memo/SemEval2020/additional_features/roberta_avg_pool_1860_test.npy\")\n",
    "nrclex_test = np.load(\"/home/memo/SemEval2020/additional_features/nrclex_1860_test.npy\")\n",
    "stanford_test = np.load(\"/home/memo/SemEval2020/additional_features/corenlp_1860_test.npy\")\n",
    "# concatenate other features in the starting only\n",
    "other_features_test = np.concatenate((toxicity_test, nrclex_test,stanford_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc96e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(df, df_test, img_embd_clip_train, img_embd_clip_test, text_embd_clip_train, text_embd_clip_test, img_embd_resnext_train, img_embd_resnext_test, text_embd_bert_train, text_embd_bert_test, additional_feat_train, additional_feat_test, thr, num_epochs, b_size, ol_n):\n",
    "    \n",
    "    n_train = int(img_embd_clip_train.shape[0]/b_size)*b_size\n",
    "    n_test = int(img_embd_clip_test.shape[0]/b_size) * b_size\n",
    "\n",
    "    # load features for CMRL\n",
    "    img_glob = img_embd_clip_train[:n_train]\n",
    "    text_glob = text_embd_clip_train[:n_train]\n",
    "    img_glob_test = img_embd_clip_test[:n_test]\n",
    "    text_glob_test = text_embd_clip_test[:n_test]\n",
    "   \n",
    "    # load features for HAN\n",
    "    img_glob_alt = img_embd_resnext_train[:n_train]\n",
    "    text_glob_alt = text_embd_bert_train[:n_train]\n",
    "    img_glob_alt_test = img_embd_resnext_test[:n_test]\n",
    "    text_glob_alt_test = text_embd_bert_test[:n_test]\n",
    "    \n",
    "    # load task specific features\n",
    "    additional_features = additional_feat_train[:n_train]\n",
    "    additional_features_test = additional_feat_test[:n_test]  \n",
    "    \n",
    "    \n",
    "    def get_adj(enco):\n",
    "      norm_enco = enco / np.linalg.norm(enco, axis=1, keepdims=True)\n",
    "      similarity_matrix = cosine_similarity(norm_enco)\n",
    "      adjacency_matrix = np.where(similarity_matrix > thr, 1, 0)\n",
    "      adj_sparse = tf.convert_to_tensor(adjacency_matrix, dtype=tf.float32)\n",
    "      return adjacency_matrix,adj_sparse\n",
    "\n",
    "    for task in ['A', 'B', 'C']:  \n",
    "        for cat in ['humour', 'sarcasm', 'offensive', 'motivational']:  \n",
    "\n",
    "            if task == 'A':\n",
    "                labels = get_train_labels(df, 'A')\n",
    "                test_labels = get_test_labels(df_test, 'A')\n",
    "\n",
    "            else:\n",
    "                labels = get_train_labels(df, task, cat)\n",
    "                test_labels = get_test_labels(df_test, task, cat) \n",
    "                \n",
    "                \n",
    "            label = labels[:n_train]\n",
    "            label_test = test_labels[:n_test]\n",
    "\n",
    "            adj_tt,adj_sparse_tt=get_adj(text_glob)\n",
    "            adj_ii,adj_sparse_ii=get_adj(img_glob)  \n",
    "            \n",
    "            adj_tt_test,adj_sparse_tt_test=get_adj(text_glob_test)\n",
    "            adj_ii_test,adj_sparse_ii_test=get_adj(img_glob_test)\n",
    "            \n",
    "\n",
    "            # clip input and clip graph input\n",
    "            image_input_clip = tf.keras.Input(shape=(512,))\n",
    "            text_input_clip = tf.keras.Input(shape=(512,))\n",
    "\n",
    "            adj_sp_ii = tf.keras.Input(shape=(b_size,),sparse=True, dtype=tf.int64)\n",
    "            adj_sp_tt = tf.keras.Input(shape=(b_size,),sparse=True, dtype=tf.int64)\n",
    "\n",
    "            #alt_clip input\n",
    "            image_input_alt = tf.keras.Input(shape=(100,2048))\n",
    "            text_input_alt = tf.keras.Input(shape=(128,768))\n",
    "\n",
    "            #toxicity_bert, nrclex, stanford_core_nlp input (merge them before input)\n",
    "            other_features = tf.keras.Input(shape=(784,))\n",
    "\n",
    "            # word_level attention on BERT text embeddings, output_size=(768,)\n",
    "            ww = tf.cast(text_input_alt, tf.float32)\n",
    "            attention_weights_text = tf.keras.layers.Dense(units=1, activation='gelu')(ww)\n",
    "            attention_weights_text = tf.squeeze(attention_weights_text, axis=-1)\n",
    "            attention_weights_text = tf.nn.softmax(attention_weights_text, axis=1)\n",
    "            attention_weighted_repr_text = tf.expand_dims(attention_weights_text, axis=-1) * ww\n",
    "            word_level_attention = tf.reduce_sum(attention_weighted_repr_text, axis=1)\n",
    "\n",
    "\n",
    "            # positional_embedding_level attention on MRCNN-X152 image embeddings, output_size=(2048,)\n",
    "            pp = tf.cast(image_input_alt, tf.float32)\n",
    "            attention_weights_image = tf.keras.layers.Dense(units=1, activation='gelu')(pp)\n",
    "            attention_weights_image = tf.squeeze(attention_weights_image, axis=-1)\n",
    "            attention_weights_image = tf.nn.softmax(attention_weights_image, axis=1)\n",
    "            attention_weighted_repr_image = tf.expand_dims(attention_weights_image, axis=-1) * pp\n",
    "            position_level_attention = tf.reduce_sum(attention_weighted_repr_image, axis=1)\n",
    "\n",
    "\n",
    "            #image clip with image graph\n",
    "            img_img   = GraphSageConv(channels=512)([image_input_clip, adj_sp_ii])\n",
    "\n",
    "            #text clip with text graph\n",
    "            text_text = GraphSageConv(channels=512)([text_input_clip, adj_sp_tt])\n",
    "\n",
    "            #image clip with text graph\n",
    "            img_text  = GraphSageConv(channels=512)([image_input_clip, adj_sp_tt])\n",
    "\n",
    "            #text clip with image graph\n",
    "            text_img  = GraphSageConv(channels=512)([text_input_clip, adj_sp_ii])\n",
    "\n",
    "            # Concatenate all\n",
    "            merge = tf.keras.layers.concatenate([img_img, text_text, img_text, text_img,\n",
    "                                    position_level_attention, word_level_attention, other_features ],axis=1)\n",
    "\n",
    "\n",
    "            final = tf.keras.layers.Dense(4618, activation=\"relu\")(merge)\n",
    "            final = tf.keras.layers.Dropout(0.2)(final)\n",
    "            final = tf.keras.layers.Dense(2048, activation=\"relu\")(final)\n",
    "            final = tf.keras.layers.Dropout(0.2)(final)\n",
    "\n",
    "            \n",
    "            task_output_layers = []\n",
    "\n",
    "            # Creating separate output layers for each task\n",
    "            \n",
    "            if task = 'B':\n",
    "                output_layers = [tf.keras.layers.Dense(3, activation=\"softmax\")(final),\n",
    "                                 tf.keras.layers.Dense(2, activation=\"softmax\")(final),\n",
    "                                 tf.keras.layers.Dense(2, activation=\"softmax\")(final),\n",
    "                                 tf.keras.layers.Dense(2, activation=\"softmax\")(final),\n",
    "                                 tf.keras.layers.Dense(2, activation=\"softmax\")(final)]\n",
    "                \n",
    "            \n",
    "            if task = 'C':\n",
    "                output_layers = [tf.keras.layers.Dense(3, activation=\"softmax\")(final),\n",
    "                                 tf.keras.layers.Dense(4, activation=\"softmax\")(final),\n",
    "                                 tf.keras.layers.Dense(4, activation=\"softmax\")(final),\n",
    "                                 tf.keras.layers.Dense(4, activation=\"softmax\")(final),\n",
    "                                 tf.keras.layers.Dense(2, activation=\"softmax\")(final)]                \n",
    "                \n",
    "\n",
    "\n",
    "            # Defining the model with multiple outputs for each task\n",
    "            model = tf.keras.models.Model(inputs=[image_input_clip, text_input_clip, adj_sp_ii, adj_sp_tt, image_input_alt, text_input_alt, other_features], outputs=task_output_layers)\n",
    "\n",
    "\n",
    "            if len(label.shape) == 1:\n",
    "                model.compile(optimizer=tf.keras.optimizers.AdamW(learning_rate=5e-6), loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "                              metrics=['accuracy', tf.keras.metrics.Precision(),tf.keras.metrics.Recall(),get_f1])\n",
    "\n",
    "            else:\n",
    "                Metrics = [tf.keras.metrics.CategoricalAccuracy(name = 'accuracy'),\n",
    "                tf.keras.metrics.Precision(name = 'precision'),\n",
    "                tf.keras.metrics.Recall(name = 'recall'),\n",
    "                get_f1\n",
    "                ]\n",
    "                model.compile(optimizer = tf.keras.optimizers.AdamW(learning_rate=5e-6),\n",
    "                               loss = 'categorical_crossentropy',\n",
    "                               metrics = Metrics)\n",
    "\n",
    "\n",
    "            for j in range(ol_n):\n",
    "                indices = np.random.permutation(text_glob.shape[0])[:b_size]\n",
    "\n",
    "                X1 = np.array([img_glob[i] for i in indices])\n",
    "                X2 = np.array([text_glob[i] for i in indices])\n",
    "\n",
    "                adj_sp_iii = tf.sparse.from_dense(get_adj(X1)[0])\n",
    "                adj_sp_ttt = tf.sparse.from_dense(get_adj(X2)[0])\n",
    "\n",
    "                A1 = np.array([img_glob_alt[i] for i in indices])\n",
    "                A2 = np.array([text_glob_alt[i] for i in indices])\n",
    "\n",
    "                other = np.array([additional_features[i] for i in indices])\n",
    "\n",
    "                X =[X1,X2,adj_sp_iii,adj_sp_ttt,A1,A2,other]\n",
    "                Y = np.array([label[i] for i in indices])\n",
    "\n",
    "                history = model.fit(X, Y, epochs = num_epochs, batch_size = b_size, verbose=2,#callbacks = [checkpoint],\n",
    "                                      shuffle = False)\n",
    "         \n",
    "\n",
    "            adj_tt_test_sliced = adj_tt_test[:n_test,:n_test]\n",
    "            adj_ii_test_sliced = adj_ii_test[:n_test,:n_test]\n",
    "\n",
    "            n1=int(text_glob_test.shape[0]/b_size)\n",
    "            y_pred = []\n",
    "\n",
    "            for i in range(n1):\n",
    "                    s = slice((i*b_size),b_size*(i+1))\n",
    "                    X1 = img_glob_test[s]\n",
    "                    X2 = text_glob_test[s]\n",
    "                    adj_sp_iii_test = tf.sparse.from_dense(get_adj(X1)[0])\n",
    "                    adj_sp_ttt_test = tf.sparse.from_dense(get_adj(X2)[0])\n",
    "\n",
    "                    A1 = img_glob_alt_test[s]\n",
    "                    A2 = text_glob_alt_test[s]\n",
    "\n",
    "                    other = additional_features_test[s]\n",
    "\n",
    "                    X =[X1,X2,adj_sp_iii_test,adj_sp_ttt_test,A1,A2,other]\n",
    "\n",
    "                    pred=model.predict(X,batch_size=b_size)\n",
    "                    y_pred.append(pred)\n",
    "\n",
    "\n",
    "            if len(label.shape) == 1:   \n",
    "                y_pred = np.asarray(y_pred)\n",
    "                y_pred = np.reshape(y_pred,(n_test,1))\n",
    "                max = 0\n",
    "                index = 0.1\n",
    "                y_test = test_labels\n",
    "\n",
    "                for i in range(9000):\n",
    "                    value = 0.1 + i*0.0001\n",
    "                    y_pred1 = np.where(y_pred>value, 1, 0)\n",
    "                    f1_macro = precision_recall_fscore_support( label_test, y_pred1 , average='macro', zero_division=1)\n",
    "                    if f1_macro[2] > max :\n",
    "                        max = f1_macro[2]\n",
    "                        index = value\n",
    "                print(max)\n",
    "                print(index)\n",
    "\n",
    "                thresh = index\n",
    "                y_pred_max = np.where(y_pred>thresh, 1, 0)\n",
    "\n",
    "                f1_macro = precision_recall_fscore_support( label_test, y_pred_max , average='macro', zero_division=1)\n",
    "                accuracy = accuracy_score( label_test, y_pred_max )\n",
    "\n",
    "                filename = \"/home/memo/SemEval2020/results/testing2.txt\"\n",
    "                with open(filename, \"a\") as file:\n",
    "                    file.write(f\"{task, cat, accuracy, f1_macro}\\n\")\n",
    "\n",
    "\n",
    "            else:\n",
    "                y_pred = np.array(y_pred)\n",
    "                y_pred = y_pred.reshape(n_test, label.shape[1])\n",
    "                one_hot_encoded = []\n",
    "\n",
    "                for pred in y_pred:\n",
    "                    one_hot = np.zeros_like(pred)\n",
    "                    max_prob_index = np.argmax(pred)\n",
    "                    one_hot[max_prob_index] = 1\n",
    "                    one_hot_encoded.append(one_hot)\n",
    "\n",
    "                y_pred_ohe = np.array(one_hot_encoded)\n",
    "                y_pred_ohe = y_pred_ohe.astype(int)\n",
    "                test_labels = test_labels[:n_test]\n",
    "                test_label = test_labels\n",
    "                y_pred1 = y_pred_ohe\n",
    "\n",
    "                accuracy = accuracy_score( test_label, y_pred1 )\n",
    "                f1_macro = precision_recall_fscore_support( test_label, y_pred1 , average='macro')\n",
    "\n",
    "                filename = \"/home/memo/SemEval2020/results/testing2.txt\"\n",
    "\n",
    "                if task == 'A':\n",
    "                    with open(filename, \"a\") as file:\n",
    "                        file.write(f\"{task, '-', accuracy, f1_macro}\\n\")\n",
    "                        break\n",
    "\n",
    "                else:\n",
    "                    with open(filename, \"a\") as file:\n",
    "                        file.write(f\"{task, cat, accuracy, f1_macro}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a3e5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_eval(dataframe, dataframe_test, img_glob, img_glob_test, text_glob, text_glob_test, img_glob_alt, img_glob_alt_test, text_glob_alt, text_glob_alt_test, additional_features, other_features_test, 0.9, 2, 120, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49092585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
