# MM-ORIENT: A Multimodal-Multitask Framework with Cross-modal Relation and Hierarchical Interactive Attention for Semantic Comprehension

![architecture](https://github.com/devraj-raghuvanshi/MM-ORIENT/assets/108318452/dcd04d11-e390-47f8-b678-ff4c3b291fc4)

## Abstract
The multimodal learning methods have constantly focused on acquiring a proficient joint multimodal represen- tation. However, the intricate fusion techniques employed to create multimodal features result in the neglect of discrimi- native information contained within the monomodal features. Moreover, monomodal representation inherently contains noise, which influences latent multimodal representations when these representations are obtained through explicit cross-modal in- teraction among different modalities. To this end, we propose a Multimodal-Multitask framework with crOss-modal Relation and hIErarchical iNteractive aTtention (MM-ORIENT) that is effective for multiple tasks. The proposed approach acquires multimodal representations in a cross-modal manner without explicit interaction between different modalities at the latent stage. To achieve this, we propose cross-modal relation graphs that reconstruct monomodal features to acquire multimodal representations. The features are reconstructed based on the node neighborhood, where the neighborhood is decided by the features of a different modality. We also propose Hierarchical Interactive Monomadal Attention (HIMA) to focus on pertinent informa- tion within a modality. While cross-modal relation graphs help comprehend high-order relationships between two modalities, HIMA helps in multitasking by learning discriminative features of individual modalities before late-fusing them. Finally, extensive experimental evaluation on three datasets demonstrates that the proposed approach effectively comprehends multimodal content for multiple tasks.

